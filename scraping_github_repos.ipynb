{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "065a2f6b",
      "metadata": {
        "id": "065a2f6b"
      },
      "source": [
        "# Top Repos For Github Topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45723548",
      "metadata": {
        "id": "45723548"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53f7d76",
      "metadata": {
        "id": "e53f7d76"
      },
      "source": [
        "### Pick a website and describe your objective\n",
        "\n",
        "- Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
        "- Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
        "- Summarize your project idea and outline your strategy in a Juptyer notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204d847e",
      "metadata": {
        "id": "204d847e"
      },
      "source": [
        "### Project Outline:\n",
        "\n",
        "- We're going to scrape https://github.com/topics\n",
        "- Get a list of topics. For each topic, we'll get the topic title, topic page URL and topic description\n",
        "- For each topic , we'll get the top 25 repositories from the topic  page\n",
        "- For each topic we'll create a CSV file in the following format:\n",
        "\n",
        "```\n",
        "RepoName,UserName,Stars,RepoURL\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3050fd17",
      "metadata": {
        "id": "3050fd17"
      },
      "source": [
        "### Use the requests library to download web pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6320fd84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6320fd84",
        "outputId": "ce3afbff-36d4-49ec-eca6-15046b728c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/topics/3d\n",
            "3d_repoInfo.csv\n",
            "0\n",
            "https://github.com/topics/ajax\n",
            "ajax_repoInfo.csv\n",
            "1\n",
            "https://github.com/topics/algorithm\n",
            "algorithm_repoInfo.csv\n",
            "2\n",
            "https://github.com/topics/amphp\n",
            "amphp_repoInfo.csv\n",
            "3\n",
            "https://github.com/topics/android\n",
            "android_repoInfo.csv\n",
            "4\n",
            "https://github.com/topics/angular\n",
            "angular_repoInfo.csv\n",
            "5\n",
            "https://github.com/topics/ansible\n",
            "ansible_repoInfo.csv\n",
            "6\n",
            "https://github.com/topics/api\n",
            "api_repoInfo.csv\n",
            "7\n",
            "https://github.com/topics/arduino\n",
            "arduino_repoInfo.csv\n",
            "8\n",
            "https://github.com/topics/aspnet\n",
            "aspnet_repoInfo.csv\n",
            "9\n",
            "https://github.com/topics/atom\n",
            "atom_repoInfo.csv\n",
            "10\n",
            "https://github.com/topics/awesome\n",
            "awesome_repoInfo.csv\n",
            "11\n",
            "https://github.com/topics/aws\n",
            "aws_repoInfo.csv\n",
            "12\n",
            "https://github.com/topics/azure\n",
            "azure_repoInfo.csv\n",
            "13\n",
            "https://github.com/topics/babel\n",
            "babel_repoInfo.csv\n",
            "14\n",
            "https://github.com/topics/bash\n",
            "bash_repoInfo.csv\n",
            "15\n",
            "https://github.com/topics/bitcoin\n",
            "bitcoin_repoInfo.csv\n",
            "16\n",
            "https://github.com/topics/bootstrap\n",
            "bootstrap_repoInfo.csv\n",
            "17\n",
            "https://github.com/topics/bot\n",
            "bot_repoInfo.csv\n",
            "18\n",
            "https://github.com/topics/c\n",
            "c_repoInfo.csv\n",
            "19\n",
            "https://github.com/topics/chrome\n",
            "chrome_repoInfo.csv\n",
            "20\n",
            "https://github.com/topics/chrome-extension\n",
            "chrome-extension_repoInfo.csv\n",
            "21\n",
            "https://github.com/topics/cli\n",
            "cli_repoInfo.csv\n",
            "22\n",
            "https://github.com/topics/clojure\n",
            "clojure_repoInfo.csv\n",
            "23\n",
            "https://github.com/topics/code-quality\n",
            "code-quality_repoInfo.csv\n",
            "24\n",
            "https://github.com/topics/code-review\n",
            "code-review_repoInfo.csv\n",
            "25\n",
            "https://github.com/topics/compiler\n",
            "compiler_repoInfo.csv\n",
            "26\n",
            "https://github.com/topics/continuous-integration\n",
            "continuous-integration_repoInfo.csv\n",
            "27\n",
            "https://github.com/topics/covid-19\n",
            "covid-19_repoInfo.csv\n",
            "28\n",
            "https://github.com/topics/cpp\n",
            "cpp_repoInfo.csv\n",
            "29\n"
          ]
        }
      ],
      "source": [
        "# library needed to scrape gut-hub\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import logging as lg\n",
        "\n",
        "lg.basicConfig(filename='file.log', level=lg.INFO)\n",
        "\n",
        "\n",
        "## class to scrape the names of the top topics in giyhub topics page\n",
        "class Github_Scrape_Topics:\n",
        "    def __init__(self):\n",
        "        self.topic_url = 'https://github.com/topics'\n",
        "    \n",
        "    def scrape_topics(self):\n",
        "        # getting response from the github topics page\n",
        "        try:\n",
        "            response = requests.get(self.topic_url)\n",
        "            # Check respone Status\n",
        "            if response.status_code !=200:\n",
        "                raise Exception(f\"Failed to load Page{self.topic_url}\")\n",
        "        except Exception as e:\n",
        "            print('error github scrape 1',str(e))\n",
        "            lg.error(str(e))\n",
        "\n",
        "\n",
        "        # parse web_page \n",
        "        try:\n",
        "            doc = BeautifulSoup(response.text,'html.parser') # using beautiful soup to parse the html page\n",
        "            topic_selection_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
        "            topic_title_tags = doc.find_all('p',{'class' : topic_selection_class}) # finding all the 'p' tags from the class containing topics_name\n",
        "\n",
        "            topic_description_class = 'f5 color-fg-muted mb-0 mt-1'\n",
        "            topic_description_tag = doc.find_all('p', {'class' : topic_description_class}) # finding all the 'p' tags from the class containing topics_description\n",
        "\n",
        "            url_class = 'no-underline flex-1 d-flex flex-column'\n",
        "            url_tag = doc.find_all('a',{'class' : url_class}) # finding all the 'a' tags from the class containing link to topic repos\n",
        "\n",
        "            topic_titles = [i.text for i in topic_title_tags] # extracting all the names from the topic_titles\n",
        "            topic_descriptions = [tags.text.strip() for tags in topic_description_tag] # extracting all the description from the topic_titles\n",
        "\n",
        "            base_url = 'https://github.com'\n",
        "            topic_urls = [base_url + tags['href'] for tags in url_tag] # extracting repos url\n",
        "        except  Exception as e:\n",
        "            print('error',e)\n",
        "            lg.error(str(e))\n",
        "\n",
        "        #creating dataframe to store all the topics_details\n",
        " \n",
        "        topics_df = pd.DataFrame({\n",
        "        'Topics': topic_titles , \n",
        "        'Description': topic_descriptions,\n",
        "        'url' : topic_urls}) \n",
        "\n",
        "        topics_df.to_csv('github_topics.csv',index=None) # saving all the extracted info into a csv file\n",
        "        return topics_df\n",
        "\n",
        "\n",
        "## class to scrape all the repos details of individual topic and saving it to csv file\n",
        "class scrape_repos:\n",
        "    def __init__(self,topic_url):\n",
        "        self.topic_url = topic_url\n",
        "    def get_topic_doc(self):\n",
        "        # Download the page\n",
        "        try:\n",
        "            print(self.topic_url)\n",
        "            response = requests.get(self.topic_url)\n",
        "            # Check respone Status\n",
        "            # if response.status_code !=200:\n",
        "            #     raise Exception(f\"Failed to load Page{self.topic_url}\")\n",
        "            # parse web_page \n",
        "            topic_doc = BeautifulSoup(response.text,'html.parser')\n",
        "            return topic_doc\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"error\",str(e))\n",
        "            lg.error(str(e))\n",
        "\n",
        "        \n",
        "    # function to parse starcounts [eg. 85k ----> 85000]\n",
        "    def parse_star_count(self,star_str):\n",
        "        try:\n",
        "            star_str = star_str.strip()\n",
        "            if star_str[-1] == 'k':\n",
        "                return int(float(star_str[:-1])*1000)\n",
        "            return int(star_str)\n",
        "        except Exception as e:\n",
        "            print('error',str(e))\n",
        "\n",
        "    def get_repo_info(self,h3_tag, star_tag):\n",
        "        \"\"\"return repository info\"\"\"\n",
        "        base_url = 'https://github.com'\n",
        "        try:\n",
        "            a_tags = h3_tag.find_all('a')\n",
        "            user_name = a_tags[0].text.strip()\n",
        "            repo_name = a_tags[1].text.strip()\n",
        "            repo_url = base_url + a_tags[1]['href']\n",
        "            stars = self.parse_star_count(star_tag.text)\n",
        "            return user_name , repo_name , stars , repo_url\n",
        "        except Exception as e:\n",
        "            print('error',str(e))\n",
        "            lg.error(str(e))\n",
        "\n",
        "    def get_topic_repos(self):\n",
        "        \n",
        "        topic_doc = self.get_topic_doc()\n",
        "        try:\n",
        "            # get the h3 tag containing username , reponame , repoURL\n",
        "            h3_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
        "            repo_tag = topic_doc.find_all('h3',{'class': h3_class})\n",
        "            # get the span tag containing all the star numbers\n",
        "            span_id = 'repo-stars-counter-star'\n",
        "            star_tag = topic_doc.find_all('span', {'id' : span_id})\n",
        "\n",
        "            repo_info_dict = {\n",
        "            'topic' : [],\n",
        "            'user_name' : [],\n",
        "            'repo_name' : [],\n",
        "            'stars' : [],\n",
        "            'repo_url' : []\n",
        "            }\n",
        "\n",
        "\n",
        "            #Get repo info\n",
        "\n",
        "            for i in range(len(repo_tag)):\n",
        "                repo_info = self.get_repo_info(repo_tag[i] , star_tag[i])\n",
        "                repo_info_dict['topic'].append(self.topic_url.split('/')[-1])\n",
        "                repo_info_dict['user_name'].append(repo_info[0])\n",
        "                repo_info_dict['repo_name'].append(repo_info[1])\n",
        "                repo_info_dict['stars'].append(repo_info[2])\n",
        "                repo_info_dict['repo_url'].append(repo_info[3])\n",
        "\n",
        "            topic_name = self.topic_url.split('/')[-1] + '_repoInfo.csv'\n",
        "\n",
        "\n",
        "            pd.DataFrame(repo_info_dict).to_csv(topic_name,index=None)\n",
        "            print(topic_name)\n",
        "        except Exception as e:\n",
        "            print('error',str(e))\n",
        "            lg.error(str(e))\n",
        "\n",
        "scrape = Github_Scrape_Topics()\n",
        "df = scrape.scrape_topics()\n",
        "topic_list = list(df.url)\n",
        "ndf = pd.DataFrame()\n",
        "c = 0\n",
        "for url in topic_list:\n",
        "    \n",
        "    repos = scrape_repos(url)\n",
        "    df = repos.get_topic_repos()\n",
        "    print(c)\n",
        "    c+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f546088e",
      "metadata": {
        "id": "f546088e"
      },
      "source": [
        "### Document and share your work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d13cdef",
      "metadata": {
        "id": "6d13cdef"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b609f1",
      "metadata": {
        "id": "23b609f1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f121f2",
      "metadata": {
        "id": "68f121f2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c569c2",
      "metadata": {
        "id": "98c569c2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "scraping-github-repos.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}